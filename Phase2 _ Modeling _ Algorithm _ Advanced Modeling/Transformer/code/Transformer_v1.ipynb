{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_v1.ipynb","provenance":[],"collapsed_sections":["EAza4i9vfXQn"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"afLEFXyHtsrv","colab_type":"text"},"source":["# Tensorflow 1버전에 맞춘 Transformer 모듈\n"]},{"cell_type":"code","metadata":{"id":"5KlI9p60gM50","colab_type":"code","outputId":"e9c2c200-2779-4f57-db1f-5875d10ccdff","executionInfo":{"status":"ok","timestamp":1578618794905,"user_tz":-540,"elapsed":149340,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H4qEJz9PgBzw","colab_type":"code","outputId":"76b6618b-009d-420b-a853-08a47a97b8f0","executionInfo":{"status":"ok","timestamp":1578618670507,"user_tz":-540,"elapsed":29365,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":539}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","!pip install --upgrade keras\n","!pip install fasttext==0.8.3\n","import fasttext\n","\n","from tensorflow import keras\n","import os\n","import sys\n","import json\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import text\n","from keras import metrics\n","from keras import backend\n","from keras.backend.tensorflow_backend import set_session\n","from keras.models import Model\n","import keras.layers as layers\n","from keras import optimizers\n","import re\n","import string"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting keras\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 24.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 4.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n","Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.5)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.3.1\n","Collecting fasttext==0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/86/ff826211bc9e28d4c371668b30b4b2c38a09127e5e73017b1c0cd52f9dfa/fasttext-0.8.3.tar.gz (73kB)\n","\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.3) (1.17.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from fasttext==0.8.3) (0.16.0)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.8.3-cp36-cp36m-linux_x86_64.whl size=1092678 sha256=fb50dc513b9fabbafc5944a411d5d3e5fb8a66534dc390aa4642eebee455b8a6\n","  Stored in directory: /root/.cache/pip/wheels/73/8e/5d/ecb50b90adaab5868ae1d8df180f31e55e85c2f055aaf2fb35\n","Successfully built fasttext\n","Installing collected packages: fasttext\n","Successfully installed fasttext-0.8.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-BElADpkqtxO","colab_type":"code","outputId":"0fb256ea-fee8-4201-f8db-ec3812ad8b8a","executionInfo":{"status":"ok","timestamp":1578618801276,"user_tz":-540,"elapsed":588,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","# config.gpu_options.per_process_gpu_memory_fraction = 0.8\n","tf.enable_eager_execution(config=config)\n","\n","print(keras.__version__)\n","print(tf.__version__)\n","\n","# keras와 tf 버전이 중요합니다\n","# Requirements.txt\n","#keras==2.1.2\n","#numpy==1.13.1\n","#fasttext==0.8.3\n","#tensorflow-gpu==1.4.1\n","\n","# 기본 코랩 버전으로도 비벼진다는 점을 발견했습니다\n","#2.2.4-tf\n","#1.15.0"],"execution_count":4,"outputs":[{"output_type":"stream","text":["2.2.4-tf\n","1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ox-nO3hM33us","colab_type":"code","outputId":"369c07ba-7cb4-4a71-abf2-81b00435ac7e","executionInfo":{"status":"ok","timestamp":1578599685892,"user_tz":-540,"elapsed":4794,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["pip list"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Package                  Version    \n","------------------------ -----------\n","absl-py                  0.9.0      \n","alabaster                0.7.12     \n","albumentations           0.1.12     \n","altair                   4.0.0      \n","asgiref                  3.2.3      \n","astor                    0.8.1      \n","astropy                  4.0        \n","atari-py                 0.2.6      \n","atomicwrites             1.3.0      \n","attrs                    19.3.0     \n","audioread                2.1.8      \n","autograd                 1.3        \n","Babel                    2.8.0      \n","backcall                 0.1.0      \n","backports.tempfile       1.0        \n","backports.weakref        1.0.post1  \n","beautifulsoup4           4.6.3      \n","bleach                   3.1.0      \n","blis                     0.2.4      \n","bokeh                    1.4.0      \n","boto                     2.49.0     \n","boto3                    1.10.47    \n","botocore                 1.13.47    \n","Bottleneck               1.3.1      \n","branca                   0.3.1      \n","bs4                      0.0.1      \n","bz2file                  0.98       \n","cachetools               4.0.0      \n","certifi                  2019.11.28 \n","cffi                     1.13.2     \n","chainer                  6.5.0      \n","chardet                  3.0.4      \n","chart-studio             1.0.0      \n","Click                    7.0        \n","cloudpickle              1.2.2      \n","cmake                    3.12.0     \n","colorlover               0.3.0      \n","community                1.0.0b1    \n","contextlib2              0.5.5      \n","convertdate              2.2.0      \n","coverage                 3.7.1      \n","coveralls                0.5        \n","crcmod                   1.7        \n","cufflinks                0.17.0     \n","cupy-cuda101             6.5.0      \n","cvxopt                   1.2.3      \n","cvxpy                    1.0.25     \n","cycler                   0.10.0     \n","cymem                    2.0.3      \n","Cython                   0.29.14    \n","daft                     0.0.4      \n","dask                     2.9.1      \n","dataclasses              0.7        \n","datascience              0.10.6     \n","decorator                4.4.1      \n","defusedxml               0.6.0      \n","descartes                1.1.0      \n","dill                     0.3.1.1    \n","distributed              1.25.3     \n","Django                   3.0.2      \n","dlib                     19.18.0    \n","dm-sonnet                1.35       \n","docopt                   0.6.2      \n","docutils                 0.15.2     \n","dopamine-rl              1.0.5      \n","earthengine-api          0.1.209    \n","easydict                 1.9        \n","ecos                     2.0.7.post1\n","editdistance             0.5.3      \n","en-core-web-sm           2.1.0      \n","entrypoints              0.3        \n","et-xmlfile               1.0.1      \n","fa2                      0.3.5      \n","fancyimpute              0.4.3      \n","fastai                   1.0.60     \n","fastcache                1.1.0      \n","fastdtw                  0.3.4      \n","fastprogress             0.2.2      \n","fastrlock                0.4        \n","fbprophet                0.5        \n","feather-format           0.4.0      \n","featuretools             0.4.1      \n","filelock                 3.0.12     \n","fix-yahoo-finance        0.0.22     \n","Flask                    1.1.1      \n","folium                   0.8.3      \n","fsspec                   0.6.2      \n","future                   0.16.0     \n","gast                     0.2.2      \n","GDAL                     2.2.2      \n","gdown                    3.6.4      \n","gensim                   3.6.0      \n","geographiclib            1.50       \n","geopy                    1.17.0     \n","gevent                   1.4.0      \n","gin-config               0.3.0      \n","glob2                    0.7        \n","google                   2.0.3      \n","google-api-core          1.15.0     \n","google-api-python-client 1.7.11     \n","google-auth              1.4.2      \n","google-auth-httplib2     0.0.3      \n","google-auth-oauthlib     0.4.1      \n","google-cloud-bigquery    1.21.0     \n","google-cloud-core        1.0.3      \n","google-cloud-datastore   1.8.0      \n","google-cloud-language    1.2.0      \n","google-cloud-storage     1.16.2     \n","google-cloud-translate   1.5.0      \n","google-colab             1.0.0      \n","google-pasta             0.1.8      \n","google-resumable-media   0.4.1      \n","googleapis-common-protos 1.6.0      \n","googledrivedownloader    0.4        \n","graph-nets               1.0.5      \n","graphviz                 0.10.1     \n","greenlet                 0.4.15     \n","grpcio                   1.15.0     \n","gspread                  3.0.1      \n","gspread-dataframe        3.0.3      \n","gunicorn                 20.0.4     \n","gym                      0.15.4     \n","h5py                     2.8.0      \n","HeapDict                 1.0.1      \n","holidays                 0.9.12     \n","html5lib                 1.0.1      \n","httpimport               0.5.18     \n","httplib2                 0.11.3     \n","humanize                 0.5.1      \n","hyperopt                 0.1.2      \n","ideep4py                 2.0.0.post3\n","idna                     2.8        \n","image                    1.5.27     \n","imageio                  2.4.1      \n","imagesize                1.2.0      \n","imbalanced-learn         0.4.3      \n","imblearn                 0.0        \n","imgaug                   0.2.9      \n","importlib-metadata       1.3.0      \n","imutils                  0.5.3      \n","inflect                  2.1.0      \n","intel-openmp             2020.0.133 \n","intervaltree             2.1.0      \n","ipykernel                4.6.1      \n","ipython                  5.5.0      \n","ipython-genutils         0.2.0      \n","ipython-sql              0.3.9      \n","ipywidgets               7.5.1      \n","itsdangerous             1.1.0      \n","jax                      0.1.52     \n","jaxlib                   0.1.36     \n","jdcal                    1.4.1      \n","jedi                     0.15.2     \n","jieba                    0.40       \n","Jinja2                   2.10.3     \n","jmespath                 0.9.4      \n","joblib                   0.14.1     \n","jpeg4py                  0.1.4      \n","jsonschema               2.6.0      \n","jupyter                  1.0.0      \n","jupyter-client           5.3.4      \n","jupyter-console          5.2.0      \n","jupyter-core             4.6.1      \n","kaggle                   1.5.6      \n","kapre                    0.1.3.1    \n","Keras                    2.2.5      \n","Keras-Applications       1.0.8      \n","Keras-Preprocessing      1.1.0      \n","keras-vis                0.4.1      \n","kfac                     0.2.0      \n","kiwisolver               1.1.0      \n","knnimpute                0.1.0      \n","librosa                  0.6.3      \n","lightgbm                 2.2.3      \n","llvmlite                 0.31.0     \n","lmdb                     0.98       \n","lucid                    0.3.8      \n","lunardate                0.2.0      \n","lxml                     4.2.6      \n","magenta                  0.3.19     \n","Markdown                 3.1.1      \n","MarkupSafe               1.1.1      \n","matplotlib               3.1.2      \n","matplotlib-venn          0.11.5     \n","mesh-tensorflow          0.1.8      \n","mido                     1.2.6      \n","mir-eval                 0.5        \n","missingno                0.4.2      \n","mistune                  0.8.4      \n","mizani                   0.6.0      \n","mkl                      2019.0     \n","mlxtend                  0.14.0     \n","more-itertools           8.0.2      \n","moviepy                  0.2.3.5    \n","mpi4py                   3.0.3      \n","mpmath                   1.1.0      \n","msgpack                  0.5.6      \n","multiprocess             0.70.9     \n","multitasking             0.0.9      \n","murmurhash               1.0.2      \n","music21                  5.5.0      \n","natsort                  5.5.0      \n","nbconvert                5.6.1      \n","nbformat                 5.0.3      \n","networkx                 2.4        \n","nibabel                  2.3.3      \n","nltk                     3.2.5      \n","notebook                 5.2.2      \n","np-utils                 0.5.12.1   \n","numba                    0.47.0     \n","numexpr                  2.7.1      \n","numpy                    1.17.5     \n","nvidia-ml-py3            7.352.0    \n","oauth2client             4.1.3      \n","oauthlib                 3.1.0      \n","okgrade                  0.4.3      \n","opencv-contrib-python    4.1.2.30   \n","opencv-python            4.1.2.30   \n","openpyxl                 2.5.9      \n","opt-einsum               3.1.0      \n","osqp                     0.6.1      \n","packaging                20.0       \n","palettable               3.3.0      \n","pandas                   0.25.3     \n","pandas-datareader        0.7.4      \n","pandas-gbq               0.11.0     \n","pandas-profiling         1.4.1      \n","pandocfilters            1.4.2      \n","parso                    0.5.2      \n","pathlib                  1.0.1      \n","patsy                    0.5.1      \n","pexpect                  4.7.0      \n","pickleshare              0.7.5      \n","Pillow                   6.2.2      \n","pip                      19.3.1     \n","pip-tools                4.2.0      \n","plac                     0.9.6      \n","plotly                   4.4.1      \n","plotnine                 0.6.0      \n","pluggy                   0.7.1      \n","portpicker               1.2.0      \n","prefetch-generator       1.0.1      \n","preshed                  2.0.1      \n","pretty-midi              0.2.8      \n","prettytable              0.7.2      \n","progressbar2             3.38.0     \n","prometheus-client        0.7.1      \n","promise                  2.3        \n","prompt-toolkit           1.0.18     \n","protobuf                 3.10.0     \n","psutil                   5.4.8      \n","psycopg2                 2.7.6.1    \n","ptyprocess               0.6.0      \n","py                       1.8.1      \n","pyarrow                  0.14.1     \n","pyasn1                   0.4.8      \n","pyasn1-modules           0.2.7      \n","pycocotools              2.0.0      \n","pycparser                2.19       \n","pydata-google-auth       0.2.1      \n","pydot                    1.3.0      \n","pydot-ng                 2.0.0      \n","pydotplus                2.0.2      \n","PyDrive                  1.3.1      \n","pyemd                    0.5.1      \n","pyglet                   1.3.2      \n","Pygments                 2.1.3      \n","pygobject                3.26.1     \n","pymc3                    3.7        \n","PyMeeus                  0.3.6      \n","pymongo                  3.10.0     \n","pymystem3                0.2.0      \n","PyOpenGL                 3.1.5      \n","pyparsing                2.4.6      \n","pypng                    0.0.20     \n","pyrsistent               0.15.7     \n","pysndfile                1.3.8      \n","PySocks                  1.7.1      \n","pystan                   2.19.1.1   \n","pytest                   3.6.4      \n","python-apt               1.6.4      \n","python-chess             0.23.11    \n","python-dateutil          2.6.1      \n","python-louvain           0.13       \n","python-rtmidi            1.3.1      \n","python-slugify           4.0.0      \n","python-utils             2.3.0      \n","pytz                     2018.9     \n","PyWavelets               1.1.1      \n","PyYAML                   3.13       \n","pyzmq                    17.0.0     \n","qtconsole                4.6.0      \n","regex                    2019.12.20 \n","requests                 2.21.0     \n","requests-oauthlib        1.3.0      \n","resampy                  0.2.2      \n","retrying                 1.3.3      \n","rpy2                     2.9.5      \n","rsa                      4.0        \n","s3fs                     0.4.0      \n","s3transfer               0.2.1      \n","scikit-image             0.16.2     \n","scikit-learn             0.22.1     \n","scipy                    1.4.1      \n","screen-resolution-extra  0.0.0      \n","scs                      2.1.1.post2\n","seaborn                  0.9.0      \n","semantic-version         2.8.4      \n","Send2Trash               1.5.0      \n","setuptools               42.0.2     \n","setuptools-git           1.2        \n","Shapely                  1.6.4.post2\n","simplegeneric            0.8.1      \n","six                      1.12.0     \n","sklearn                  0.0        \n","sklearn-pandas           1.8.0      \n","smart-open               1.9.0      \n","snowballstemmer          2.0.0      \n","sortedcontainers         2.1.0      \n","spacy                    2.1.9      \n","Sphinx                   1.8.5      \n","sphinxcontrib-websupport 1.1.2      \n","SQLAlchemy               1.3.12     \n","sqlparse                 0.3.0      \n","srsly                    1.0.1      \n","stable-baselines         2.2.1      \n","statsmodels              0.10.2     \n","sympy                    1.1.1      \n","tables                   3.4.4      \n","tabulate                 0.8.6      \n","tblib                    1.6.0      \n","tensor2tensor            1.14.1     \n","tensorboard              1.15.0     \n","tensorboardcolab         0.0.22     \n","tensorflow               1.15.0     \n","tensorflow-datasets      1.3.2      \n","tensorflow-estimator     1.15.1     \n","tensorflow-gan           2.0.0      \n","tensorflow-hub           0.7.0      \n","tensorflow-metadata      0.15.2     \n","tensorflow-privacy       0.2.2      \n","tensorflow-probability   0.7.0      \n","termcolor                1.1.0      \n","terminado                0.8.3      \n","testpath                 0.4.4      \n","text-unidecode           1.3        \n","textblob                 0.15.3     \n","textgenrnn               1.4.1      \n","tflearn                  0.3.2      \n","Theano                   1.0.4      \n","thinc                    7.0.8      \n","toolz                    0.10.0     \n","torch                    1.3.1      \n","torchsummary             1.5.1      \n","torchtext                0.3.1      \n","torchvision              0.4.2      \n","tornado                  4.5.3      \n","tqdm                     4.28.1     \n","traitlets                4.3.3      \n","tweepy                   3.6.0      \n","typing                   3.6.6      \n","typing-extensions        3.6.6      \n","tzlocal                  1.5.1      \n","umap-learn               0.3.10     \n","uritemplate              3.0.1      \n","urllib3                  1.24.3     \n","vega-datasets            0.8.0      \n","wasabi                   0.6.0      \n","wcwidth                  0.1.8      \n","webencodings             0.5.1      \n","Werkzeug                 0.16.0     \n","wheel                    0.33.6     \n","widgetsnbextension       3.5.1      \n","wordcloud                1.5.0      \n","wrapt                    1.11.2     \n","xarray                   0.14.1     \n","xgboost                  0.90       \n","xkit                     0.0.0      \n","xlrd                     1.1.0      \n","xlwt                     1.3.0      \n","yellowbrick              0.9.1      \n","zict                     1.0.0      \n","zipp                     0.6.0      \n","zmq                      0.0.0      \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x9_s7EZLgzYs","colab_type":"text"},"source":["## 모듈\n","\n"]},{"cell_type":"code","metadata":{"id":"x7ROesnogJ1I","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import text\n","\n","# Preprocess - 토크나이저와 전처리를 맡는 클래스! 순서대로 진행 추천\n","# 먼저 3개 인자 넣음 (Q와 A는 pandas series type일 때 작동확인)\n","# 1. buildTokenizer : 토크나이저 리턴(save 가능)\n","# 2. tokenize_and_filter : 토크나이징 된 input과 output 리턴\n","\n","class Preprocess:\n","    def __init__(self):\n","        super().__init__()\n","    def buildTokenizer(self, questions, answers):\n","        corpus = questions + answers\n","        tk = Tokenizer(filters='!\"#$%&()*+,-.s:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=False) # / 만 살렸다, 소문자변환 안한다.\n","        tk.fit_on_texts(corpus)\n","        VOCAB_SIZE = len(tk.word_index) + 1\n","        self.START_TOKEN, self.END_TOKEN = [VOCAB_SIZE], [VOCAB_SIZE + 1]\n","        self.VOCAB_SIZE = VOCAB_SIZE + 2   \n","        return tk\n","\n","    def saveTokenizer(self, directory, tokenizer):\n","        tk_json = tokenizer.to_json()\n","        with open(directory, 'w', encoding='UTF-8-sig') as f:\n","            f.write(json.dumps(vars(tokenizer), ensure_ascii = False))\n","    def loadTokenzier(self, directory):\n","        with open(directory, encoding='UTF-8-sig') as fh:\n","            data = json.load(fh)\n","        tk = Tokenizer()\n","        key = list(data.keys())\n","        for i in key:\n","            setattr(tk, i, data[i])\n","        VOCAB_SIZE = len(tk.word_index) + 1 \n","        self.START_TOKEN, self.END_TOKEN = [VOCAB_SIZE], [VOCAB_SIZE + 1]\n","        self.VOCAB_SIZE = VOCAB_SIZE + 2  \n","        return tk\n","\n","    # 리스트 형태를 토큰 모델에 넣어주려고 text로 만듦\n","    def list2text(self, x):\n","        return [e for s in x for e in s]\n","\n","    # Tokenize, filter and pad sentences\n","    def tokenize_and_filter(self, questions, answers, tokenizer, MAX_LENGTH=30):\n","        tokenized_inputs, tokenized_outputs = [], []\n","\n","        for (sentence1, sentence2) in zip(questions, answers):\n","            # tokenize sentence\n","            sentence1 = [self.START_TOKEN] + tokenizer.texts_to_sequences(sentence1) + [self.END_TOKEN]\n","            sentence2 = [self.START_TOKEN] + tokenizer.texts_to_sequences(sentence2) + [self.END_TOKEN]\n","            # check tokenized sentence max length\n","            if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n","                tokenized_inputs.append(self.list2text(sentence1))\n","                tokenized_outputs.append(self.list2text(sentence2))\n","\n","        # pad tokenized sentences\n","        tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","                            tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","        tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","                            tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","        return tokenized_inputs, tokenized_outputs\n","    \n","    def buildDataset(self, inputs, outputs, BATCH_SIZE, BUFFER_SIZE=20000):\n","        dataset = tf.data.Dataset.from_tensor_slices((\n","            {\n","                'inputs': inputs,\n","                'dec_inputs': outputs[:, :-1]\n","            },\n","            {\n","                'outputs': outputs[:, 1:]\n","            },\n","        ))\n","        dataset = dataset.cache()\n","        dataset = dataset.repeat()\n","        dataset = dataset.shuffle(BUFFER_SIZE)\n","        dataset = dataset.batch(BATCH_SIZE)\n","        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","        return dataset\n","\n","############################\n","#### MultiHeadAttention ####\n","############################\n","def scaled_dot_product_attention(query, key, value, mask):\n","    \"\"\"Calculate the attention weights. \"\"\"\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","    # scale matmul_qk\n","    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","    logits = matmul_qk / tf.math.sqrt(depth) # scaling \n","\n","    # add the mask to zero out padding tokens\n","    if mask is not None:\n","        logits += (mask * -1e9)\n","\n","    # softmax is normalized on the last axis (seq_len_k)\n","    attention_weights = tf.nn.softmax(logits, axis=-1) \n","\n","    output = tf.matmul(attention_weights, value)\n","\n","    return output\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, name=\"multi_head_attention\", **kwargs):\n","        super(MultiHeadAttention, self).__init__(**kwargs)\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.depth = self.d_model // self.num_heads\n","        assert d_model % self.num_heads == 0\n","        self.query_dense = tf.keras.layers.Dense(units=d_model)\n","        self.key_dense = tf.keras.layers.Dense(units=d_model)\n","        self.value_dense = tf.keras.layers.Dense(units=d_model)\n","        self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","    def get_config(self):\n","        config = super(MultiHeadAttention, self).get_config()\n","        config.update({\n","                        \"num_heads\":self.num_heads, \n","                        \"d_model\":self.d_model,\n","                       })\n","        return config\n","\n","    def split_heads(self, inputs, batch_size):\n","        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        query, key, value, mask = inputs['query'], inputs['key'], inputs[ \n","                                    'value'], inputs['mask']\n","        batch_size = tf.shape(query)[0]\n","        # linear layers\n","        query = self.query_dense(query)\n","        key = self.key_dense(key)\n","        value = self.value_dense(value)\n","        # split heads\n","        query = self.split_heads(query, batch_size)\n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","        # scaled dot-product attention\n","        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","        # concatenation of heads\n","        concat_attention = tf.reshape(scaled_attention,\n","                                        (batch_size, -1, self.d_model))\n","        outputs = self.dense(concat_attention)\n","        return outputs\n","\n","#################\n","#### Masking ####\n","#################\n","def create_padding_mask(x):\n","    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","    # (batch_size, 1, 1, sequence length)\n","    return mask[:, tf.newaxis, tf.newaxis, :]\n","def create_look_ahead_mask(x):\n","    seq_len = tf.shape(x)[1]\n","    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    padding_mask = create_padding_mask(x)\n","    return tf.maximum(look_ahead_mask, padding_mask)\n","\n","######################\n","#### Pos Encoding ####\n","######################\n","class PositionalEncoding(tf.keras.layers.Layer):\n","    def __init__(self, position, d_model, name='PositionalEncoding', **kwargs):\n","        super(PositionalEncoding, self).__init__(**kwargs)\n","        self.d_model = d_model\n","        self.position = position\n","        self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","    def get_angles(self, position, i, d_model):\n","        angles = 1 / tf.pow(tf.constant(10000,dtype='float32'), (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","        return position * angles\n","\n","    def positional_encoding(self, position, d_model):\n","        angle_rads = self.get_angles(\n","                            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","                            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","                            d_model=d_model)\n","        # apply sin to even index in the array\n","        sines = tf.math.sin(angle_rads[:, 0::2])\n","        # apply cos to odd index in the array\n","        cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","        pos_encoding = tf.concat([sines, cosines], axis=-1)\n","        pos_encoding = pos_encoding[tf.newaxis, ...]\n","        return tf.cast(pos_encoding, tf.float32)\n","\n","    def get_config(self):\n","        config = super(PositionalEncoding, self).get_config().copy()\n","        config.update({\n","                        'd_model': self.d_model,\n","                        'position': self.position\n","                        })\n","        return config\n","\n","    def call(self, inputs):\n","        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","###########################\n","#### 인코더 내부 Layer ####\n","###########################\n","def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    attention = MultiHeadAttention(\n","        d_model, num_heads, name=\"attention\")({\n","            'query': inputs,\n","            'key': inputs,\n","            'value': inputs,\n","            'mask': padding_mask\n","        })\n","    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n","\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","  \n","##############################################\n","#### 임베딩 + Pos Encoding + 인코더 Layer ####\n","##############################################\n","def encoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name=\"encoder\"):\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    for i in range(num_layers):\n","        outputs = encoder_layer(\n","            units=units,\n","            d_model=d_model,\n","            num_heads=num_heads,\n","            dropout=dropout,\n","            name=\"encoder_layer_{}\".format(i),\n","        )([outputs, padding_mask])\n","\n","    return tf.keras.Model(\n","        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","###########################\n","#### 디코더 내부 Layer ####\n","###########################\n","def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(inputs={\n","                    'query': inputs,\n","                    'key': inputs,\n","                    'value': inputs,\n","                    'mask': look_ahead_mask\n","                })\n","    attention1 = tf.keras.layers.LayerNormalization(\n","    epsilon=1e-6)(attention1 + inputs)\n","\n","    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(inputs={\n","                    'query': attention1,\n","                    'key': enc_outputs,\n","                    'value': enc_outputs,\n","                    'mask': padding_mask\n","                })\n","    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","    attention2 = tf.keras.layers.LayerNormalization(\n","    epsilon=1e-6)(attention2 + attention1)\n","\n","    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n","    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","    outputs = tf.keras.layers.LayerNormalization(\n","    epsilon=1e-6)(outputs + attention2)\n","\n","    return tf.keras.Model(\n","                    inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","                    outputs=outputs,\n","                    name=name)\n","\n","###################################################\n","#### 임베딩 + Pos Encoding + 디코더 내부 Layer ####\n","###################################################\n","def decoder(vocab_size,\n","            num_layers,\n","            units,\n","            d_model,\n","            num_heads,\n","            dropout,\n","            name='decoder'):\n","    inputs = tf.keras.Input(shape=(None,), name='inputs')\n","    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n","    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","\n","    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","    for i in range(num_layers):\n","        outputs = decoder_layer(\n","                    units=units,\n","                    d_model=d_model,\n","                    num_heads=num_heads,\n","                    dropout=dropout,\n","                    name='decoder_layer_{}'.format(i),\n","                )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","    return tf.keras.Model(\n","                        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","                        outputs=outputs,\n","                        name=name)\n","\n","#######################################\n","#### 인코더 + 디코더 = Transformer ####\n","#######################################\n","def transformer(vocab_size,\n","                num_layers,\n","                units,\n","                d_model,\n","                num_heads,\n","                dropout,\n","                name=\"transformer\"):\n","    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","    enc_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask, output_shape=(1, 1, None),\n","        name='enc_padding_mask')(inputs)\n","    # mask the future tokens for decoder inputs at the 1st attention block\n","    look_ahead_mask = tf.keras.layers.Lambda(\n","        create_look_ahead_mask,\n","        output_shape=(1, None, None),\n","        name='look_ahead_mask')(dec_inputs)\n","    # mask the encoder outputs for the 2nd attention block\n","    dec_padding_mask = tf.keras.layers.Lambda(\n","        create_padding_mask, output_shape=(1, 1, None),\n","        name='dec_padding_mask')(inputs)\n","\n","    enc_outputs = encoder(\n","                    vocab_size=vocab_size,\n","                    num_layers=num_layers,\n","                    units=units,\n","                    d_model=d_model,\n","                    num_heads=num_heads,\n","                    dropout=dropout,\n","                )(inputs=[inputs, enc_padding_mask])\n","\n","    dec_outputs = decoder(\n","                    vocab_size=vocab_size,\n","                    num_layers=num_layers,\n","                    units=units,\n","                    d_model=d_model,\n","                    num_heads=num_heads,\n","                    dropout=dropout,\n","                )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","    \n","#######################################\n","######### Loss & LearningRate #########\n","#######################################\n","def customLoss(MAX_LENGTH, name='customLoss'):\n","    def loss_function(y_true, y_pred, name='loss_function'):\n","        y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True, reduction='none')(y_true, y_pred)\n","\n","        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","        loss = tf.multiply(loss, mask)\n","        return tf.reduce_mean(loss)\n","    return loss_function\n","    \n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000, name='CustomSchedule', **kwargs):\n","        super(CustomSchedule, self).__init__(**kwargs)\n","\n","        self.d_m = d_model\n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","    def get_config(self):\n","        config = {\n","                  'd_model': self.d_m,\n","                  'warmup_steps': self.warmup_steps,\n","                 }\n","        return config \n","\n","def custom_accuracy(MAX_LENGTH, name='custom_accuracy'):\n","    def accuracy(y_true, y_pred):\n","        y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","    return accuracy\n","\n","#### 예측 ####\n","def transform(sentence, analyzer, anl_type):\n","    if anl_type == 'Konlpy_morphs':\n","        return [[i] for i in  analyzer.morphs(sentence)]\n","    elif anl_type == 'Konlpy_pos':\n","        return [[i] for i in  analyzer.pos(sentence, join=True)]\n","    elif anl_type == 'Khaiii':\n","        sentence = [str(i).split('\\t')[1] for i in  analyzer.analyze(sentence)]\n","        return [e for s in sentence for e in s.split(' + ')]\n","    else:\n","        print('use Konlpy or Khaiii')\n","\n","def evaluate(model, tokenizer, sentence, analyzer, anl_type, MAX_LENGTH=30):\n","    VOCAB_SIZE = len(tokenizer.word_index) + 1\n","    START_TOKEN, END_TOKEN = [VOCAB_SIZE], [VOCAB_SIZE + 1]\n","    VOCAB_SIZE += 2\n","\n","    sentence = [START_TOKEN] + tokenizer.texts_to_sequences(transform(sentence, analyzer, anl_type)) + [END_TOKEN]\n","    sentence = tf.expand_dims([e for s in sentence for e in s],axis=0)\n","    output = tf.expand_dims(START_TOKEN, 0)\n","    for i in range(MAX_LENGTH):\n","        predictions = model(inputs=[sentence, output], training=False)\n","\n","    # select the last word from the seq_len dimension\n","        predictions = predictions[:, -1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    # return the result if the predicted_id is equal to the end token\n","        if tf.equal(predicted_id, END_TOKEN[0]) is None:\n","            break\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    output = tf.squeeze(output, axis=0)\n","    return tokenizer.sequences_to_texts(\n","                        [[i] for i in output.numpy()])\n","\n","def pos2text(token_list):\n","    pure_input = [i[:i.index('/')] for i in token_list]\n","    # 띄어쓰기 교정\n","    # Josa : 조사, PreEomi : 선어말어미, Eomi : 어미, Suffix : 접미사\n","    pos_list = ['Josa','PreEomi','Eomi','Suffix']\n","    for i in range(len(token_list)):\n","        p = token_list[i]\n","        if p[p.index('/')+1:] in pos_list:\n","            # '/토큰'으로 만들어서 나중에 앞 단어와 붙일 거임\n","            pure_input[i] = '/'+pure_input[i]\n","    pure_text = ' '.join(pure_input)\n","    pure_text = pure_text.replace(' /','')\n","\n","    # Rule 추가\n","    pure_text = pure_text.replace('중 증','중증')\n","    pure_text = pure_text.replace(' 은','은')\n","    pure_text = pure_text.replace(' 로','로')\n","    pure_text = pure_text.replace(' 와','와')\n","\n","\n","    return pure_text\n","\n","def token2text(token_list):\n","    return ' '.join(token_list)\n","def text2token(x, analyzer):\n","    return analyzer.pos(x, join=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAza4i9vfXQn","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"AZh1cUGjgujI","colab_type":"code","outputId":"e6f548d8-0d46-40c4-8483-5983364760d3","executionInfo":{"status":"ok","timestamp":1578498559559,"user_tz":-540,"elapsed":1044,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# 사람마다 디렉토리 다름 주의\n","tk_dir = '/content/drive/My Drive/ToBigs/탄시리(Tanhsiri)/Phase2 : Modeling & Algorithm & Advanced Modeling/Transformer_준걸_윤종/tokenizer_data_v1.json'\n","corpus = '/content/drive/My Drive/ToBigs/탄시리(Tanhsiri)/Phase1 : Data Collection & Preprocessing /Data/통합데이터/Konlpy_data.pickle'\n","df = pd.read_pickle(corpus)\n","\n","# Khaiii 썼을 떄 쓰는 거\n","# def cut(x):\n","#     return [i[:i.index('/')+3] for i in x]\n","# questions = df['pre_Question'].apply(cut)\n","# answers = df['pre_Answer'].apply(cut)\n","# questions = df['Q_kh']\n","# answers = df['A_kh']\n","\n","# Konlpy 썼을 떄 쓰는 거\n","questions = df['Q_noPos']\n","answers = df['A_noPos']\n","\n","MAX_LENGTH = 30\n","preprocess = Preprocess()\n","\n","# 옵션1) 새로운 토크나이저 만들기\n","# tokenizer = preprocess.buildTokenizer(questions, answers)\n","# questions_seq, answers_seq = preprocess.tokenize_and_filter(questions, answers, tokenizer, MAX_LENGTH)\n","# 옵션2) 토크나이저 저장\n","# preprocess.saveTokenizer(tk_dir, tokenizer)\n","# 옵션3) 기존 토크나이저 로드\n","tokenizer = preprocess.loadTokenzier(tk_dir)\n","questions_seq, answers_seq = preprocess.tokenize_and_filter(questions, answers, tokenizer, MAX_LENGTH)\n","\n","print('Vocab size: {}'.format(preprocess.VOCAB_SIZE))\n","print('Number of samples: {}'.format(len(questions_seq)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocab size: 3663\n","Number of samples: 1043\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w8dlsZ-UhCLd","colab_type":"code","outputId":"e6e589f0-5e04-4202-a4f9-e8ce2a6c01dd","executionInfo":{"status":"ok","timestamp":1578498560332,"user_tz":-540,"elapsed":1044,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# dataset\n","BATCH_SIZE = 64\n","dataset = preprocess.buildDataset(questions_seq, answers_seq, BATCH_SIZE)\n","dataset"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ({inputs: (?, 30), dec_inputs: (?, 29)}, {outputs: (?, 29)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"AEBsQAGXhErg","colab_type":"code","outputId":"b9f0f23d-8e21-42f0-e5db-cc417156a55b","executionInfo":{"status":"error","timestamp":1578498564388,"user_tz":-540,"elapsed":1039,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":633}},"source":["# tf.keras.backend.clear_session()\n","\n","# parameters\n","NUM_LAYERS = 2\n","D_MODEL = 512\n","NUM_HEADS = 8\n","UNITS = 512\n","DROPOUT = 0.2\n","VOCAB_SIZE = preprocess.VOCAB_SIZE\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    units=UNITS,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","learning_rate = 1e-4\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","model.compile(optimizer=optimizer, loss=customLoss(MAX_LENGTH), metrics=[custom_accuracy(MAX_LENGTH)])\n","\n","# 옵션) 모델 plot\n","# tf.keras.utils.plot_model(model, to_file='transformer.png', show_shapes=True)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7378b2a14e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_HEADS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     dropout=DROPOUT)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-aba6a5fcca6d>\u001b[0m in \u001b[0;36mtransformer\u001b[0;34m(vocab_size, num_layers, units, d_model, num_heads, dropout, name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     enc_padding_mask = tf.keras.layers.Lambda(\n\u001b[1;32m    345\u001b[0m         \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         name='enc_padding_mask')(inputs)\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;31m# mask the future tokens for decoder inputs at the 1st attention block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     look_ahead_mask = tf.keras.layers.Lambda(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, function, mask, arguments, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Get layer name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'output_shape')"]}]},{"cell_type":"code","metadata":{"id":"FZ0MWRHPhG6G","colab_type":"code","outputId":"66dff4b0-5363-4edb-eabe-2ba3bd436cb3","executionInfo":{"status":"ok","timestamp":1578496415881,"user_tz":-540,"elapsed":116762,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["steps = len(questions_seq) // BATCH_SIZE\n","EPOCHS = 100\n","print(f'MAX_LENGTH : {MAX_LENGTH}, EPOCHS : {EPOCHS}')\n","\n","model.fit(dataset, epochs=EPOCHS, steps_per_epoch=steps)\n","\n","# 옵션) verbose = 0 모드일 떄 최종 train acc 찍어보는 거\n","# test_scores = model.evaluate(dataset, verbose=2)\n","# print('Test loss:', test_scores[0])\n","# print('Test accuracy:', test_scores[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MAX_LENGTH : 30, EPOCHS : 100\n","Train on 16 steps\n","Epoch 1/100\n","16/16 [==============================] - 5s 292ms/step - loss: 4.4979 - accuracy: 6.7349e-05\n","Epoch 2/100\n","16/16 [==============================] - 1s 59ms/step - loss: 4.3856 - accuracy: 0.0049\n","Epoch 3/100\n","16/16 [==============================] - 1s 59ms/step - loss: 4.2734 - accuracy: 0.0315\n","Epoch 4/100\n","16/16 [==============================] - 1s 58ms/step - loss: 4.1639 - accuracy: 0.0346\n","Epoch 5/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.9699 - accuracy: 0.0345\n","Epoch 6/100\n","16/16 [==============================] - 1s 61ms/step - loss: 4.0237 - accuracy: 0.0348\n","Epoch 7/100\n","16/16 [==============================] - 1s 59ms/step - loss: 4.0084 - accuracy: 0.0365\n","Epoch 8/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.8663 - accuracy: 0.0400\n","Epoch 9/100\n","16/16 [==============================] - 1s 58ms/step - loss: 3.7261 - accuracy: 0.0404\n","Epoch 10/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.7474 - accuracy: 0.0441\n","Epoch 11/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.6321 - accuracy: 0.0459\n","Epoch 12/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.5124 - accuracy: 0.0457\n","Epoch 13/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.5237 - accuracy: 0.0488\n","Epoch 14/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.5958 - accuracy: 0.0490\n","Epoch 15/100\n","16/16 [==============================] - 1s 61ms/step - loss: 3.3713 - accuracy: 0.0510\n","Epoch 16/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.3496 - accuracy: 0.0548\n","Epoch 17/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.2544 - accuracy: 0.0618\n","Epoch 18/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.1834 - accuracy: 0.0649\n","Epoch 19/100\n","16/16 [==============================] - 1s 60ms/step - loss: 3.2089 - accuracy: 0.0724\n","Epoch 20/100\n","16/16 [==============================] - 1s 59ms/step - loss: 3.0942 - accuracy: 0.0777\n","Epoch 21/100\n","16/16 [==============================] - 1s 58ms/step - loss: 3.0428 - accuracy: 0.0827\n","Epoch 22/100\n","16/16 [==============================] - 1s 58ms/step - loss: 3.0531 - accuracy: 0.0907\n","Epoch 23/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.9383 - accuracy: 0.0932\n","Epoch 24/100\n","16/16 [==============================] - 1s 57ms/step - loss: 2.9214 - accuracy: 0.1000\n","Epoch 25/100\n","16/16 [==============================] - 1s 59ms/step - loss: 2.9293 - accuracy: 0.1069\n","Epoch 26/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.8014 - accuracy: 0.1071\n","Epoch 27/100\n","16/16 [==============================] - 1s 59ms/step - loss: 2.7999 - accuracy: 0.1140\n","Epoch 28/100\n","16/16 [==============================] - 1s 59ms/step - loss: 2.7173 - accuracy: 0.1203\n","Epoch 29/100\n","16/16 [==============================] - 1s 57ms/step - loss: 2.6460 - accuracy: 0.1269\n","Epoch 30/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.6275 - accuracy: 0.1360\n","Epoch 31/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.5916 - accuracy: 0.1419\n","Epoch 32/100\n","16/16 [==============================] - 1s 60ms/step - loss: 2.5106 - accuracy: 0.1460\n","Epoch 33/100\n","16/16 [==============================] - 1s 57ms/step - loss: 2.4437 - accuracy: 0.1487\n","Epoch 34/100\n","16/16 [==============================] - 1s 59ms/step - loss: 2.4162 - accuracy: 0.1551\n","Epoch 35/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.3904 - accuracy: 0.1647\n","Epoch 36/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.3243 - accuracy: 0.1670\n","Epoch 37/100\n","16/16 [==============================] - 1s 59ms/step - loss: 2.2690 - accuracy: 0.1755\n","Epoch 38/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.1934 - accuracy: 0.1762\n","Epoch 39/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.0701 - accuracy: 0.1815\n","Epoch 40/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.0255 - accuracy: 0.1847\n","Epoch 41/100\n","16/16 [==============================] - 1s 58ms/step - loss: 2.0632 - accuracy: 0.1979\n","Epoch 42/100\n","16/16 [==============================] - 1s 57ms/step - loss: 1.9262 - accuracy: 0.2008\n","Epoch 43/100\n","16/16 [==============================] - 1s 60ms/step - loss: 1.8918 - accuracy: 0.2079\n","Epoch 44/100\n","16/16 [==============================] - 1s 58ms/step - loss: 1.8892 - accuracy: 0.2157\n","Epoch 45/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.7997 - accuracy: 0.2211\n","Epoch 46/100\n","16/16 [==============================] - 1s 58ms/step - loss: 1.7054 - accuracy: 0.2217\n","Epoch 47/100\n","16/16 [==============================] - 1s 60ms/step - loss: 1.7037 - accuracy: 0.2328\n","Epoch 48/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.6526 - accuracy: 0.2413\n","Epoch 49/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.6401 - accuracy: 0.2491\n","Epoch 50/100\n","16/16 [==============================] - 1s 57ms/step - loss: 1.5488 - accuracy: 0.2550\n","Epoch 51/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.4636 - accuracy: 0.2597\n","Epoch 52/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.4689 - accuracy: 0.2801\n","Epoch 53/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.3311 - accuracy: 0.2807\n","Epoch 54/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.2874 - accuracy: 0.2884\n","Epoch 55/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.3023 - accuracy: 0.3037\n","Epoch 56/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.2481 - accuracy: 0.3134\n","Epoch 57/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.2227 - accuracy: 0.3209\n","Epoch 58/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.1080 - accuracy: 0.3249\n","Epoch 59/100\n","16/16 [==============================] - 1s 59ms/step - loss: 1.0871 - accuracy: 0.3348\n","Epoch 60/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.9918 - accuracy: 0.3473\n","Epoch 61/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.9704 - accuracy: 0.3572\n","Epoch 62/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.9230 - accuracy: 0.3682\n","Epoch 63/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.8620 - accuracy: 0.3718\n","Epoch 64/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.8402 - accuracy: 0.3882\n","Epoch 65/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.7891 - accuracy: 0.3891\n","Epoch 66/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.7601 - accuracy: 0.4066\n","Epoch 67/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.7002 - accuracy: 0.4100\n","Epoch 68/100\n","16/16 [==============================] - 1s 61ms/step - loss: 0.6686 - accuracy: 0.4255\n","Epoch 69/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.6186 - accuracy: 0.4375\n","Epoch 70/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.6091 - accuracy: 0.4505\n","Epoch 71/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.5549 - accuracy: 0.4550\n","Epoch 72/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.4968 - accuracy: 0.4671\n","Epoch 73/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.4599 - accuracy: 0.4670\n","Epoch 74/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.4312 - accuracy: 0.4764\n","Epoch 75/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.3979 - accuracy: 0.4825\n","Epoch 76/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.3752 - accuracy: 0.4923\n","Epoch 77/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.3371 - accuracy: 0.4969\n","Epoch 78/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.2965 - accuracy: 0.4948\n","Epoch 79/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.2703 - accuracy: 0.5027\n","Epoch 80/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.2628 - accuracy: 0.5161\n","Epoch 81/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.2286 - accuracy: 0.5046\n","Epoch 82/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.2158 - accuracy: 0.5242\n","Epoch 83/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.1913 - accuracy: 0.5147\n","Epoch 84/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.1746 - accuracy: 0.5189\n","Epoch 85/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.1573 - accuracy: 0.5311\n","Epoch 86/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.1399 - accuracy: 0.5293\n","Epoch 87/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.1271 - accuracy: 0.5227\n","Epoch 88/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.1191 - accuracy: 0.5351\n","Epoch 89/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.1072 - accuracy: 0.5346\n","Epoch 90/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.0920 - accuracy: 0.5265\n","Epoch 91/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.0952 - accuracy: 0.5391\n","Epoch 92/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.0836 - accuracy: 0.5335\n","Epoch 93/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.0746 - accuracy: 0.5468\n","Epoch 94/100\n","16/16 [==============================] - 1s 59ms/step - loss: 0.0667 - accuracy: 0.5351\n","Epoch 95/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.0604 - accuracy: 0.5253\n","Epoch 96/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.0590 - accuracy: 0.5325\n","Epoch 97/100\n","16/16 [==============================] - 1s 60ms/step - loss: 0.0511 - accuracy: 0.5296\n","Epoch 98/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.0480 - accuracy: 0.5294\n","Epoch 99/100\n","16/16 [==============================] - 1s 57ms/step - loss: 0.0476 - accuracy: 0.5318\n","Epoch 100/100\n","16/16 [==============================] - 1s 58ms/step - loss: 0.0438 - accuracy: 0.5393\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fe08c993d30>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"PPH5RB_EhRdW","colab_type":"code","colab":{}},"source":["# 모델 저장\n","path = '/content/drive/My Drive/ToBigs/탄시리(Tanhsiri)/Phase2 : Modeling & Algorithm & Advanced Modeling/Transformer_준걸_윤종/tf_model_v1.h5'\n","model.save(path)\n","del model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-4WykdKkPKy","colab_type":"text"},"source":["## Eval"]},{"cell_type":"code","metadata":{"id":"kyodxLwckP8m","colab_type":"code","outputId":"769d3669-9256-4c3d-f636-51a5a42af086","executionInfo":{"status":"ok","timestamp":1578618868982,"user_tz":-540,"elapsed":53838,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Install Konlpy\n","!apt-get update\n","!apt-get install g++ openjdk-8-jdk \n","!pip3 install konlpy"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.174)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.174)] [Connecting to security.u\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.174)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.174)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.174)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [5 Re\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [6 Re\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","\r                                                                               \r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rGet:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 k\r0% [Waiting for headers] [8 InRelease 14.2 kB/88.7 kB 16%] [Connecting to ppa.l\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [8 InRelease 14.2 kB/88.7 kB \r                                                                               \rGet:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [81.6 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [30.4 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,749 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [21.8 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,325 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [803 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [777 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [35.5 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,074 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [10.8 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,241 B]\n","Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [844 kB]\n","Fetched 7,030 kB in 3s (2,011 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n","g++ set to manually installed.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n","  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jre x11-utils\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin mesa-utils\n","The following NEW packages will be installed:\n","  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n","  libatk-wrapper-java-jni libxxf86dga1 openjdk-8-jdk openjdk-8-jre x11-utils\n","0 upgraded, 8 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 4,954 kB of archives.\n","After this operation, 13.3 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u232-b09-0ubuntu1~18.04.1 [69.8 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u232-b09-0ubuntu1~18.04.1 [1,618 kB]\n","Fetched 4,954 kB in 1s (3,427 kB/s)\n","Selecting previously unselected package libxxf86dga1:amd64.\n","(Reading database ... 145674 files and directories currently installed.)\n","Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Selecting previously unselected package fonts-dejavu-core.\n","Preparing to unpack .../1-fonts-dejavu-core_2.37-1_all.deb ...\n","Unpacking fonts-dejavu-core (2.37-1) ...\n","Selecting previously unselected package fonts-dejavu-extra.\n","Preparing to unpack .../2-fonts-dejavu-extra_2.37-1_all.deb ...\n","Unpacking fonts-dejavu-extra (2.37-1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../3-x11-utils_7.7+3build1_amd64.deb ...\n","Unpacking x11-utils (7.7+3build1) ...\n","Selecting previously unselected package libatk-wrapper-java.\n","Preparing to unpack .../4-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n","Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n","Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n","Preparing to unpack .../5-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n","Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n","Selecting previously unselected package openjdk-8-jre:amd64.\n","Preparing to unpack .../6-openjdk-8-jre_8u232-b09-0ubuntu1~18.04.1_amd64.deb ...\n","Unpacking openjdk-8-jre:amd64 (8u232-b09-0ubuntu1~18.04.1) ...\n","Selecting previously unselected package openjdk-8-jdk:amd64.\n","Preparing to unpack .../7-openjdk-8-jdk_8u232-b09-0ubuntu1~18.04.1_amd64.deb ...\n","Unpacking openjdk-8-jdk:amd64 (8u232-b09-0ubuntu1~18.04.1) ...\n","Setting up fonts-dejavu-core (2.37-1) ...\n","Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Setting up fonts-dejavu-extra (2.37-1) ...\n","Setting up x11-utils (7.7+3build1) ...\n","Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n","Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n","Setting up openjdk-8-jre:amd64 (8u232-b09-0ubuntu1~18.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n","Setting up openjdk-8-jdk:amd64 (8u232-b09-0ubuntu1~18.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 152kB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/62/0f312d578e0165e9b5e8fcae0291f7ee83783b3805f59071006b21229d55/JPype1-0.7.1.tar.gz (481kB)\n","\u001b[K     |████████████████████████████████| 491kB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 14.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n","Collecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Building wheels for collected packages: JPype1\n","  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for JPype1: filename=JPype1-0.7.1-cp36-cp36m-linux_x86_64.whl size=2437883 sha256=51126613afed0a940c3310e33247e6752261ef34d09be9c9c635dd6dfa3f6245\n","  Stored in directory: /root/.cache/pip/wheels/b0/49/6a/4393ef8542c00becf80691bd242693db9e263d6e499323a984\n","Successfully built JPype1\n","Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tR7AT-ikkdqz","colab_type":"code","colab":{}},"source":["\n","\n","# 모델 로드\n","path_1 = '/content/drive/My Drive/ToBigs/탄시리(Tanhsiri)/Phase2 : Modeling & Algorithm & Advanced Modeling/Transformer_준걸_윤종/tf_model_noPos3.h5'\n","MAX_LENGTH = 30\n","custom_objects = {'PositionalEncoding':PositionalEncoding,\n","                'MultiHeadAttention':MultiHeadAttention,\n","                'CustomSchedule':CustomSchedule,\n","                'loss_function':customLoss(MAX_LENGTH),\n","                'accuracy':custom_accuracy(MAX_LENGTH), \n","                'create_padding_mask':create_padding_mask,\n","                'backend':backend, \n","                'tf':tf}\n","model_1 = keras.models.load_model(path_1, custom_objects=custom_objects)\n","\n","# 토크나이저 로드\n","tk_dir_1 = '/content/drive/My Drive/ToBigs/탄시리(Tanhsiri)/Phase2 : Modeling & Algorithm & Advanced Modeling/Transformer_준걸_윤종/tokenizer_data_Konlpy_noPos.json'\n","\n","preprocess1 = Preprocess()\n","tokenizer_1 = preprocess1.loadTokenzier(tk_dir_1)\n","\n","# 토크나이저 로드하면 모든 key,value가 string으로 들어감 나중에 토큰을 텍스트로 \n","# 복원할 때 정상적으로 구동하기 위해서 index_word는 key를 int로 바꿔줌\n","tokenizer_1.index_word = {int(k):v for k,v in tokenizer_1.index_word.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2gMqO3Ya1So","colab_type":"code","colab":{}},"source":["from konlpy.tag import Okt\n","t = Okt()\n","\n","# 띄어쓰기\n","\n","class Space:\n","    def __init__(self):\n","        # 띄어쓰기\n","        self.adam_half = optimizers.Adam(lr=0.0005)\n","        self.dic_kor = self.load_bin()\n","        self.model_kor = self.load_space_model()\n","        self.threshold_kor = 0.5\n","        self.overlap = 30\n","\n","    def load_bin(self):\n","        return fasttext.load_model('/content/drive/My Drive/model_kor.bin')\n","    def load_space_model(self):\n","        return keras.models.load_model('/content/drive/My Drive/model_kor.hdf5')\n","\n","    def pred_correction_rnn(self, sent,model,dic,maxlen,wdim):\n","        conv = np.zeros((1,maxlen,wdim,1))\n","        rnn = np.zeros((1,maxlen,wdim))\n","        charcount = -1\n","        for j in range(len(sent)):\n","          print('j')\n","          if j<maxlen and sent[j]!=' ':\n","            charcount=charcount+1\n","            conv[0][charcount,:,0]=dic[sent[j]]\n","            rnn[0][charcount,:]=dic[sent[j]]\n","        z = model.predict([conv,rnn])[0]\n","        sent_raw = ''\n","        count_char=-1\n","        lastpoint=-1\n","        lastchar=-1\n","        for j in range(len(sent)):\n","          if sent[j]!=' ':\n","            count_char=count_char+1\n","            sent_raw = sent_raw+sent[j]\n","            if z[count_char]>self.threshold_kor:\n","              sent_raw = sent_raw+' '\n","              if j<self.overlap:\n","                lastpoint=len(sent_raw)\n","                lastchar=j\n","        return sent_raw, lastpoint, lastchar\n","\n","    def kor_spacing(self, s):\n","        if len(s)<self.overlap:\n","          temp,lp,lc = self.pred_correction_rnn(s,self.model_kor,self.dic_kor,100,100)\n","          z = temp+\"\\n\"\n","        else:\n","          z=''\n","          start=0\n","          while start<len(s):\n","            if start+self.overlap<len(s):\n","              print('sent', s[start:start+2*self.overlap])\n","              temp,lp,lc =self.pred_correction_rnn(s[start:start+2*self.overlap],self.model_kor,self.dic_kor,100,100)\n","              temp=temp[:lp]\n","            else:\n","              temp,lp,lc =self.pred_correction_rnn(s[start:],self.model_kor,self.dic_kor,100,100)\n","              lc = self.overlap\n","            z = z+temp\n","            start=start+lc+1\n","          z = z+\"\\n\"\n","        return z\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1ZF9ap6FHe_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":307},"outputId":"5a5eeacf-e324-4582-dfc7-0ad48c383f57","executionInfo":{"status":"error","timestamp":1578621129510,"user_tz":-540,"elapsed":683,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}}},"source":["space = Space()"],"execution_count":22,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-5d2dea748992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-cd890c516ea8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 띄어쓰기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic_kor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_space_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold_kor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-cd890c516ea8>\u001b[0m in \u001b[0;36mload_bin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/model_kor.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_space_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/model_kor.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'fasttext' has no attribute 'load'"]}]},{"cell_type":"code","metadata":{"id":"PkBdoztKkt4I","colab_type":"code","outputId":"ecfbdc2b-44a0-4093-b255-bf3ee92035bb","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1578620191411,"user_tz":-540,"elapsed":4047,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}}},"source":["# Konlpy\n","\n","# while True:\n","#     sentence = input()\n","#     if sentence == 'quit':\n","#         break\n","sentence = '장애인도신체검사받나요'\n","prediction_1 = evaluate(model_1, tokenizer_1, sentence, t, anl_type='Konlpy_morphs', MAX_LENGTH=30)\n","new_text_2 = space.kor_spacing(token2text(prediction_1))\n","# print(f'Q : {sentence}')\n","# print(f'Konlpy : {token2text(prediction_1)}')\n","print(f'After Spaceing : {new_text_2}')\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["sent  장애인 은 병역 이 면제 되나 장애인 등록 자 중 병역 판정 검사 대상자 의 경우 병역 판정 검사 를 받아\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","sent  중 병역 판정 검사 대상자 의 경우 병역 판정 검사 를 받아야 합니다  를 받아야 합니다    \n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","j\n","After Spaceing : 장애인은 병역이 면제되나 장애인 등록자 중병역 판정검사 대상자의 경우병역판정 검사를 받아야 합니다를 받아야 합니다\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fMeOKTaNGeT5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9554d418-8533-469b-8b5d-5f69a9e672e9","executionInfo":{"status":"ok","timestamp":1578620480200,"user_tz":-540,"elapsed":674,"user":{"displayName":"신윤종","photoUrl":"","userId":"04746276170955697226"}}},"source":["list(space.dic_kor.words)\n","# space.model_kor"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사랑하든',\n"," '했나',\n"," '했을까봐',\n"," '축하금',\n"," '봉이씨랑',\n"," '연인',\n"," '라고도',\n"," '가져오라고',\n"," '대답부터',\n"," '아무사이',\n"," '그만이니까',\n"," '학교폭력',\n"," '됐',\n"," '만노군',\n"," '군포를',\n"," '그랬다니까',\n"," '얻게',\n"," '생각함',\n"," '더러운',\n"," '괘씸죄로',\n"," '선생께서',\n"," '있었다면서',\n"," '두달에',\n"," '일천한',\n"," '되느니',\n"," '공약으로',\n"," '못합니까',\n"," '만나면서',\n"," '좋아하지두',\n"," '옮긴',\n"," '꼼짝두',\n"," '따라갈게요',\n"," '공맹의',\n"," '참',\n"," '코피도',\n"," '나뭇잎',\n"," '휴가예요',\n"," '가서라도',\n"," '가둔',\n"," '사람하고',\n"," '애쓰셨어요',\n"," '전화한거야',\n"," '그런뜻이',\n"," '아침입니다',\n"," '부러우세요',\n"," '시작했잖아',\n"," '지경사',\n"," '모두요',\n"," '명월이',\n"," '가볍고',\n"," '노려보면',\n"," '속이다니',\n"," '팔만',\n"," '달랍니다',\n"," '설거지까지',\n"," '만만찮게',\n"," '사랑하진',\n"," '정도가',\n"," '김미숙',\n"," '스키장에서',\n"," '해결되냐',\n"," '제법이구나',\n"," '땅에',\n"," '부자면서',\n"," '아셔',\n"," '대주는',\n"," '애정표현',\n"," '환',\n"," '계솔이',\n"," '본영을',\n"," '궈서',\n"," '존대를',\n"," '박혁권이',\n"," '내밀어야',\n"," '봉수가',\n"," '많음',\n"," '미현언니한테',\n"," '부탁해라',\n"," '싸인회',\n"," '엄격하게',\n"," '국화빵',\n"," '좋아했냐',\n"," '위험하다고',\n"," '명왕성',\n"," '회수',\n"," '쓸일이',\n"," '안끝났어',\n"," '멈춰서고',\n"," '녀석들도',\n"," '기막히다',\n"," '다섯배',\n"," '헤아리시어',\n"," '어머니에',\n"," '김종서가',\n"," '잡아내',\n"," '생겼수',\n"," '난리냐구',\n"," '미디어',\n"," '편집장님이랑',\n"," '열악한',\n"," '위협하여',\n"," '안되겠냐구',\n"," '말밖에',\n"," '일한',\n"," '정류장에서',\n"," '휴일이',\n"," '좋게도',\n"," '고백이나',\n"," '선배야',\n"," '마라야',\n"," '잘자',\n"," '맛인가',\n"," '헐벗고',\n"," '울주',\n"," '불쌍해',\n"," '상상의',\n"," '대학교수',\n"," '지사로',\n"," '다쳤어',\n"," '주든지',\n"," '아무리요',\n"," '보충수업은',\n"," '누룩을',\n"," '반듯한',\n"," '장안',\n"," '화랑은',\n"," '정치인의',\n"," '감독이랑',\n"," '총을',\n"," '이문제',\n"," '웬놈이야',\n"," '막노동을',\n"," '종년',\n"," '하셨지',\n"," '건지나',\n"," '피곤하잖아',\n"," '괘씸하구',\n"," '적응하기',\n"," '원손마마를',\n"," '교회',\n"," '같소이다',\n"," '연진아',\n"," '올라간다구',\n"," '대륙유통',\n"," '김일환이',\n"," '노래라두',\n"," '다녀오더니',\n"," '지나가면',\n"," '잘되는지',\n"," '데려와서',\n"," '피곤하거든요',\n"," '보리쌀',\n"," '신상이',\n"," '군량미는',\n"," '먹어보면',\n"," '들어간다면',\n"," '단곈',\n"," '말랐지',\n"," '장소도',\n"," '왔더라고',\n"," '시생을',\n"," '뿐이었어',\n"," '내나',\n"," '썼나봐',\n"," '숙',\n"," '혼자야',\n"," '싶다구요',\n"," '살았는지도',\n"," '방이지',\n"," '적몰하고',\n"," '힘내야지',\n"," '썼어야',\n"," '하십니다요',\n"," '의사에',\n"," '병원장',\n"," '김과장',\n"," '지까짓게',\n"," '소리치지',\n"," '나누겠습니다',\n"," '있으모',\n"," '하나에서',\n"," '피할수',\n"," '않겠다구',\n"," '법의관은',\n"," '불편하면',\n"," '의미하는지',\n"," '재예에',\n"," '헛다리',\n"," '했을텐데요',\n"," '다음주엔',\n"," '두시지요',\n"," '자책이',\n"," '신지현이야',\n"," '빙그레',\n"," '깨어나세요',\n"," '싶던데',\n"," '출혈도',\n"," '공간에서',\n"," '유태인이',\n"," '통금을',\n"," '산더미같은',\n"," '뻥을',\n"," '계시냐',\n"," '열세를',\n"," '들어가세나',\n"," '어떨까요',\n"," '찬성이다',\n"," '다치게만',\n"," '그리운',\n"," '근거를',\n"," '병명이',\n"," '남아도는',\n"," '악바리',\n"," '엊그제부터',\n"," '중시하는',\n"," '했던건데',\n"," '고장나지',\n"," '포기는',\n"," '간지럽고',\n"," '아줌씨가',\n"," '장인어른의',\n"," '잠깐이라두',\n"," '본론부터',\n"," '직접적으로',\n"," '선대의',\n"," '원통하옵니다',\n"," '잘되긴',\n"," '회사원',\n"," '지푸라기',\n"," '들어왔음',\n"," '맞는지',\n"," '역',\n"," '않아서야',\n"," '감정들이',\n"," '돕겠소',\n"," '동고비',\n"," '사장님한텐',\n"," '털어주고',\n"," '부러웠어요',\n"," '이원호의',\n"," '우리집두',\n"," '까져서',\n"," '그런일을',\n"," '서운하긴',\n"," '글마',\n"," '자술서를',\n"," '마마님을',\n"," '장만해서',\n"," '일본에',\n"," '빠뜨리고',\n"," '자궁을',\n"," '나갔다니까',\n"," '해치려는',\n"," '끝에는',\n"," '여러분들도',\n"," '삼부',\n"," '차유란씨',\n"," '여린게',\n"," '몇푼이나',\n"," '열쇤',\n"," '선비님은',\n"," '보좌할',\n"," '노니',\n"," '대위',\n"," '일어난다니까',\n"," '사이냐구요',\n"," '성구는',\n"," '저잣거리에서',\n"," '아까부터',\n"," '원후마마께',\n"," '피워야',\n"," '했는가',\n"," '선영이',\n"," '조언',\n"," '빤스가',\n"," '아인가',\n"," '안당과',\n"," '만나놓고',\n"," '조치는',\n"," '죽어주면',\n"," '바꿨습니다',\n"," '야명주를',\n"," '늦춰야',\n"," '소문이라도',\n"," '나란히',\n"," '드려야죠',\n"," '아시죠',\n"," '다홍치마라구',\n"," '한푼',\n"," '공부야',\n"," '굳지',\n"," '변술녀',\n"," '상황이다',\n"," '두십시오',\n"," '안되다니요',\n"," '듣겠소',\n"," '일으킨',\n"," '노민국',\n"," '기다리던',\n"," '죽겠구나',\n"," '피해자에게',\n"," '알아보시겠어요',\n"," '해주려구',\n"," '기면',\n"," '다림질',\n"," '기다리잖아',\n"," '왕손아',\n"," '윤미주',\n"," '항암제를',\n"," '구휼미를',\n"," '부담없이',\n"," '연재를',\n"," '장모님한테',\n"," '뭐해서',\n"," '믿지도',\n"," '냄새와',\n"," '부동산으로',\n"," '못했대',\n"," '색깔두',\n"," '당선될',\n"," '에이즈',\n"," '유명해지고',\n"," '아파',\n"," '묻는거',\n"," '남해에서',\n"," '동물적',\n"," '애인이야',\n"," '청바지에',\n"," '이백여',\n"," '처음이에요',\n"," '화순아',\n"," '정부장',\n"," '맛있었어',\n"," '추관으로',\n"," '터지냐',\n"," '뒤집어쓸',\n"," '멍들게',\n"," '킴',\n"," '알아보겠습니다',\n"," '동탁이가',\n"," '자들이',\n"," '악착을',\n"," '간판을',\n"," '걸려두',\n"," '뱀이다',\n"," '며느리감으로',\n"," '섭섭하네',\n"," '찾아봐줘',\n"," '나오지',\n"," '미치면',\n"," '지연이는',\n"," '불좀',\n"," '자식이라도',\n"," '이나라에',\n"," '따논',\n"," '납시셨사옵니까',\n"," '봤다니',\n"," '체력을',\n"," '지하철에서',\n"," '어려선',\n"," '까짓껏',\n"," '타협하지',\n"," '실망하는',\n"," '늙지',\n"," '재밌어',\n"," '공모전에',\n"," '우키타',\n"," '마찬',\n"," '신지현은',\n"," '팔리지도',\n"," '생명도',\n"," '구단에서',\n"," '미희',\n"," '것이다만',\n"," '찌르네',\n"," '혈소판',\n"," '네년의',\n"," '유란이',\n"," '처삽니다',\n"," '없으셨죠',\n"," '거울',\n"," '사주려구',\n"," '묻어야',\n"," '일년두',\n"," '식으론',\n"," '잡생각이',\n"," '당근이죠',\n"," '무릎이',\n"," '재밌던데',\n"," '띵겨',\n"," '한국에',\n"," '날리구',\n"," '희귀',\n"," '입어본',\n"," '스캔들까지',\n"," '받았데',\n"," '계백을',\n"," '현관문이',\n"," '찍어보면',\n"," '집집이',\n"," '살리는데',\n"," '목표도',\n"," '해인씨하고',\n"," '어떻구요',\n"," '부적이',\n"," '김창수',\n"," '됐는데',\n"," '죽겠거든요',\n"," '철기야',\n"," '배신자야',\n"," '데려다줬어',\n"," '출발하면서',\n"," '남들앞에서',\n"," '장이의',\n"," '수양이',\n"," '해결해줄',\n"," '돌아가라니까',\n"," '한귀로',\n"," '그라몬',\n"," '밀었다',\n"," '그런건지',\n"," '나갔구만',\n"," '들온대',\n"," '간동맥과',\n"," '진대인',\n"," '빠리에서',\n"," '실연이라도',\n"," '영양가',\n"," '불효',\n"," '해보',\n"," '꺄악',\n"," '촬영',\n"," '금성이',\n"," '기회잖아',\n"," '유진이와',\n"," '모공이형화',\n"," '이것이오',\n"," '은질',\n"," '태순',\n"," '오뎅이',\n"," '청한다고',\n"," '강동석씨가',\n"," '성격은',\n"," '요양원에',\n"," '무심했던',\n"," '기저귀',\n"," '아빠래',\n"," '끊겼어',\n"," '넘는다구',\n"," '민수연',\n"," '그만두겠습니다',\n"," '백옥같이',\n"," '보이차',\n"," '발뒤꿈치가',\n"," '껀은',\n"," '컴퓨터는',\n"," '계셨다면서요',\n"," '겪을',\n"," '기척은',\n"," '기생집에',\n"," '풀풀',\n"," '그런거잖아',\n"," '남겼어요',\n"," '다닌다구',\n"," '찢었는데',\n"," '반응을',\n"," '밝히지',\n"," '질문하면',\n"," '시작했으니까',\n"," '행위의',\n"," '어시스턴트',\n"," '하라는',\n"," '인간들',\n"," '형철의',\n"," '알았지요',\n"," '지금두',\n"," '드리려던',\n"," '어긋나면',\n"," '수수께끼를',\n"," '지승돈의',\n"," '단무지만',\n"," '비하면',\n"," '후궁들과',\n"," '들어왔다는',\n"," '감자두',\n"," '패주고',\n"," '걸친',\n"," '뽀뽀해',\n"," '아깝잖아',\n"," '감으시고',\n"," '양단간에',\n"," '지구를',\n"," '이선우양이',\n"," '밀린',\n"," '꼬라질',\n"," '않았냐구요',\n"," '느낌이면',\n"," '진정해',\n"," '입었으니',\n"," '한치',\n"," '연기력을',\n"," '내눈',\n"," '우여란',\n"," '짚으면',\n"," '챙기러',\n"," '집이니까',\n"," '귀영을',\n"," '에이든은',\n"," '쓰듯',\n"," '맡겼으니',\n"," '윤선생',\n"," '떠돌아다니는',\n"," '원가',\n"," '끝장이라구',\n"," '틀렸냐',\n"," '실을',\n"," '우르크',\n"," '미치겠습니다',\n"," '중전마마에',\n"," '안되지이',\n"," '마치고',\n"," '했으면서도',\n"," '잡혔다구',\n"," '생각해보시오',\n"," '스톱해',\n"," '부사장한테',\n"," '거래해',\n"," '가르치는데',\n"," '상상이',\n"," '헌테',\n"," '앓은',\n"," '닮았습니다',\n"," '여섯이면',\n"," '그렇지요',\n"," '갚아야',\n"," '그것밖엔',\n"," '묻었냐',\n"," '오시기전에',\n"," '준혁아',\n"," '카이스트',\n"," '곳이나',\n"," '파바박',\n"," '모르겠나',\n"," '일년',\n"," '진실과',\n"," '안되몬',\n"," '닌자들이',\n"," '괄호',\n"," '쳐들어와서',\n"," '깜깜해서',\n"," '아름다움을',\n"," '화가들',\n"," '흔들린다',\n"," '개똥이',\n"," '주먹질은',\n"," '들어와야',\n"," '가산을',\n"," '세나씨한테',\n"," '자신으로',\n"," '고르기가',\n"," '귀남이하고',\n"," '생선이나',\n"," '후보님의',\n"," '대회를',\n"," '기분이나',\n"," '에디',\n"," '정말이죠',\n"," '볼까나',\n"," '암만요',\n"," '같잖아요',\n"," '것이니까',\n"," '쫓겨나기',\n"," '들으면',\n"," '뭔지는',\n"," '가출했다',\n"," '행위는',\n"," '레이져',\n"," '돼나',\n"," '싸야',\n"," '안하는데',\n"," '못하는가',\n"," '낙랑공주가',\n"," '재능은',\n"," '중이오',\n"," '다혜양',\n"," '출연료도',\n"," '년산',\n"," '여봉이',\n"," '우겼어',\n"," '왕자를',\n"," '순대는',\n"," '함께하는',\n"," '손가락까지',\n"," '뭐하겠소',\n"," '강동희씨는',\n"," '수호천사',\n"," '훔쳐다',\n"," '까불',\n"," '당신만큼',\n"," '능청은',\n"," '로맨스를',\n"," '애리가',\n"," '정상회담',\n"," '크신',\n"," '나가던지',\n"," '가뭄에',\n"," '그러기가',\n"," '알았기',\n"," '년처럼',\n"," '테이블을',\n"," '칼질은',\n"," '생크림이',\n"," '작정하지',\n"," '강론을',\n"," '골절',\n"," '했다지만',\n"," '누우세요',\n"," '언성을',\n"," '입국자',\n"," '놀이방에',\n"," '운전두',\n"," '짝사랑하던',\n"," '명단이야',\n"," '다소곳이',\n"," '취민가',\n"," '에도에서',\n"," '교육부',\n"," '음악으로',\n"," '엠비에이',\n"," '장난치냐',\n"," '공으로',\n"," '없어졌거든요',\n"," '좋게',\n"," '윤',\n"," '박혀',\n"," '시월',\n"," '확신해요',\n"," '무덤속에',\n"," '꿈이었거든',\n"," '근처에다',\n"," '해주지',\n"," '백이',\n"," '계를',\n"," '길이면',\n"," '형이라두',\n"," '배웠는데',\n"," '아가는',\n"," '거라두',\n"," '아기집이',\n"," '빠른데',\n"," '났었잖아',\n"," '상사면',\n"," '소개해줄',\n"," '그런적',\n"," '김성일의',\n"," '최상질의',\n"," '드럽네',\n"," '썽',\n"," '만이요',\n"," '계집인데',\n"," '고자',\n"," '예외는',\n"," '고맙다아',\n"," '반항하고',\n"," '뭐있어요',\n"," '국내는',\n"," '치자구',\n"," '아슈',\n"," '역공을',\n"," '색깔별로',\n"," '거절하셔도',\n"," '땜시',\n"," '그림이나',\n"," '씨에프',\n"," '유헌이',\n"," '한입만',\n"," '포기안해',\n"," '나왔고',\n"," '방금전',\n"," '책임져',\n"," '나금순씨',\n"," '아인기라',\n"," '어쩌자구',\n"," '눈치예요',\n"," '지나치다',\n"," '신혼여행을',\n"," '수아도',\n"," '세금을',\n"," '평소처럼',\n"," '도총관대감의',\n"," '떠난다고',\n"," '대구',\n"," '앗싸아',\n"," '돌아오셨어요',\n"," '우경씨',\n"," '말들을',\n"," '드는데요',\n"," '안가봐도',\n"," '강군으로',\n"," '뭐라고요',\n"," '누나도',\n"," '몇시지',\n"," '떨리긴',\n"," '계곡에',\n"," '선생님은',\n"," '아니되네',\n"," '관심도',\n"," '먹어둬',\n"," '뭉쳐야',\n"," '주소하고',\n"," '알아갈',\n"," '처한테',\n"," '책임져야할',\n"," '조건두',\n"," '브로콜리',\n"," '내놓던',\n"," '칼바람이',\n"," '못마땅하신',\n"," '대상이야',\n"," '근데도',\n"," '영혼이라도',\n"," '차릴테니까',\n"," '나가실까요',\n"," '미친갱이',\n"," '마마께옵서',\n"," '봤다는데',\n"," '중요했어',\n"," '지난번처럼',\n"," '좍좍',\n"," '이탈리아에서',\n"," '버티신',\n"," '트렌드',\n"," '계셔유',\n"," '먹이는데',\n"," '평양성에서',\n"," '나와계세요',\n"," '이십년전에',\n"," '아프다고',\n"," '벅벅',\n"," '웃다가',\n"," '장사치의',\n"," '다녀오라구',\n"," '둥지',\n"," '포격을',\n"," '죽기도',\n"," '물러간',\n"," '낙엽',\n"," '형수님을',\n"," '부주방장님',\n"," '쉴려구',\n"," '숙일',\n"," '책임자로',\n"," '수십억',\n"," '나쁘구',\n"," '용서하시구려',\n"," '안보이는거야',\n"," '들어오다가',\n"," '키워놓고',\n"," '최교수가',\n"," '혼나겠다',\n"," '반대를',\n"," '좋아했었는데',\n"," '안오지',\n"," '시집만',\n"," '렌지',\n"," '쓸수도',\n"," '던지면서',\n"," '포장해주세요',\n"," '안했',\n"," '고서방이랑',\n"," '절개해서',\n"," '있는대루',\n"," '힘든건지',\n"," '정리했는데',\n"," '그림에',\n"," '안나와두',\n"," '두나야',\n"," '가지고들',\n"," '갖다주라고',\n"," '범인이라면',\n"," '오셔야',\n"," '불안하구나',\n"," '날개가',\n"," '은은히',\n"," '받아들일',\n"," '만나가지구',\n"," '불가능하다는',\n"," '목적입니다',\n"," '유진씨도',\n"," '지긋지긋해요',\n"," '불안하구',\n"," '수군에겐',\n"," '가스배달',\n"," '매달리게',\n"," '보위하라',\n"," '기대하는',\n"," '실크',\n"," '놀라면서',\n"," '소식만',\n"," '없었지만',\n"," '내말을',\n"," '사정해도',\n"," '신혼여행',\n"," '살라는데',\n"," '잡아들이고',\n"," '쪽박',\n"," '미안합니다',\n"," '살피겠습니다',\n"," '도리다',\n"," '곰국',\n"," '알아볼테니까',\n"," '잡아줘요',\n"," '사부인께서',\n"," '도주하는',\n"," '영부인',\n"," '잠복하고',\n"," '여편네를',\n"," '바쁠텐데',\n"," '올라가봐',\n"," '부치는데',\n"," '빡빡해',\n"," '진주에',\n"," '준건',\n"," '신호음',\n"," '만해투금',\n"," '몇번째',\n"," '끼어들게',\n"," '쿨한',\n"," '살수록',\n"," '만들어본',\n"," '낯',\n"," '바랐던',\n"," '부작용을',\n"," '혜림과',\n"," '맞았단',\n"," '삼송백화점을',\n"," '화나서',\n"," '것들을',\n"," '해한',\n"," '천만다행이다',\n"," '반대해도',\n"," '검사란',\n"," '할수록',\n"," '레어',\n"," '싶었겠죠',\n"," '소장의',\n"," '지켜주겠다',\n"," '선재씬',\n"," '돋지',\n"," '됐다마',\n"," '상단에',\n"," '엉덩방아를',\n"," '해줄라고',\n"," '열어보고',\n"," '아이재',\n"," '교전을',\n"," '시작했어요',\n"," '안한거',\n"," '선택해도',\n"," '삼십년이',\n"," '따내면',\n"," '송이야',\n"," '도검류를',\n"," '대부분은',\n"," '하나라도',\n"," '생겼다니',\n"," '하거든요',\n"," '몇분',\n"," '않을만큼',\n"," '스타일이야',\n"," '옥체가',\n"," '입원',\n"," '모욕해',\n"," '참을성이',\n"," '지어야',\n"," '나으리께오선',\n"," '드렸사옵니다',\n"," '간호해',\n"," '돌아옵니다',\n"," '모후를',\n"," '불빛',\n"," '이상형을',\n"," '장래가',\n"," '얼짱',\n"," '놀랐다니까',\n"," '건강해진',\n"," '잔돈',\n"," '몰표를',\n"," '만약이라고',\n"," '차버린',\n"," '청국에',\n"," '말하겠네',\n"," '졸업할때까지',\n"," '차이를',\n"," '안했구나',\n"," '살아야할',\n"," '사이에',\n"," '지시로',\n"," '년만이야',\n"," '받으셔야죠',\n"," '무리해서',\n"," '꼬이면',\n"," '여자아이는',\n"," '본지',\n"," '우얄',\n"," '여대',\n"," '간대니',\n"," '관계',\n"," '지는요',\n"," '돌아오니',\n"," '장혜성을',\n"," '돌려요',\n"," '기자로서',\n"," '고마웠다구',\n"," '인사드려',\n"," '하기엔',\n"," '받칠',\n"," '무시하긴',\n"," '증명이',\n"," '측근에서',\n"," '면천시켜',\n"," '요즈음',\n"," '강의실에',\n"," '때다',\n"," '만나준',\n"," '내자원을',\n"," '전세',\n"," '이러는게요',\n"," '노력을',\n"," '년생',\n"," '수달피',\n"," '사흘에',\n"," '찾아보자구',\n"," '않았사옵니까',\n"," '아가씨에',\n"," '탐내는',\n"," '다됐는데',\n"," '닦았어요',\n"," '취했나부다',\n"," '맡기마',\n"," '이사들이',\n"," '건강하십시오',\n"," '따위',\n"," '하셨으면서',\n"," '놈인데요',\n"," '식혤',\n"," '연쇄살인',\n"," '종소리',\n"," '보재요',\n"," '될것이라',\n"," '들어볼까',\n"," '성일이',\n"," '맞아가면서',\n"," '경비가',\n"," '회사나',\n"," '같으셔',\n"," '젊으셨을',\n"," '시숙이',\n"," '그짓을',\n"," '아저씨요',\n"," '과장님처럼',\n"," '신상',\n"," '웃겼어',\n"," '분장',\n"," '과부가',\n"," '나오믄',\n"," '천만에요',\n"," '현규한테',\n"," '우아한',\n"," '그렇겠네',\n"," '통하는데',\n"," '잠두',\n"," '없을꺼다',\n"," '타봤어요',\n"," '아버지께서',\n"," '여자나',\n"," '능통한',\n"," '나갔',\n"," '하룻동안',\n"," '일세',\n"," '못하겠군요',\n"," '겠구만',\n"," '고모를',\n"," '언제까지요',\n"," '고약하게',\n"," '늙었어요',\n"," '잊으셨습니까',\n"," '업구',\n"," '차지헌',\n"," '예약한',\n"," '설악산으로',\n"," '유명하잖아요',\n"," '가라는데',\n"," '후져요',\n"," '구질스럽게',\n"," '억울함을',\n"," '쌌다구',\n"," '심통이',\n"," '한숨',\n"," '아멘',\n"," ...]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"pC-LMFEeaC5a","colab_type":"code","colab":{}},"source":["장애인도 신체 검사 받나요"],"execution_count":0,"outputs":[]}]}